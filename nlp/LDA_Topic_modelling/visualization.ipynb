{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>virginamerica</td>\n",
       "      <td>virginamerica its really aggressive to blast o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>united</td>\n",
       "      <td>united still no refund or word via dm please r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>southwest</td>\n",
       "      <td>southwestair still waiting just hit one hourso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>delta</td>\n",
       "      <td>jetblue they werent on any flight they just ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>usairways</td>\n",
       "      <td>usairways  is there a better time to call my f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         reviews\n",
       "virginamerica  virginamerica its really aggressive to blast o...\n",
       "united         united still no refund or word via dm please r...\n",
       "southwest      southwestair still waiting just hit one hourso...\n",
       "delta          jetblue they werent on any flight they just ca...\n",
       "usairways      usairways  is there a better time to call my f..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_clean = pd.read_pickle('pickle/df_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only verb text instead of complete word set from dataframe df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def verb(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the verbs.'''\n",
    "    #is_noun_vb = lambda pos: pos[:2] == 'NN' or pos[:2] == 'VB'\n",
    "    is_verb = lambda pos: pos[:2] == 'VB'\n",
    "    tokenized = word_tokenize(text)\n",
    "    verb = [word for (word, pos) in pos_tag(tokenized) if is_verb(pos)] \n",
    "    return ' '.join(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>virginamerica</td>\n",
       "      <td>blast faces have pay didnt have flying is flew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>united</td>\n",
       "      <td>united resolve cancelled was tripunited lack d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>southwest</td>\n",
       "      <td>waiting hit cancelled flighted atl am was rebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>delta</td>\n",
       "      <td>werent came informed is are be found was had c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>usairways</td>\n",
       "      <td>is call is need change be be speakusairways hu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         reviews\n",
       "virginamerica  blast faces have pay didnt have flying is flew...\n",
       "united         united resolve cancelled was tripunited lack d...\n",
       "southwest      waiting hit cancelled flighted atl am was rebo...\n",
       "delta          werent came informed is are be found was had c...\n",
       "usairways      is call is need change be be speakusairways hu..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_verb = pd.DataFrame(data_clean.reviews.apply(verb))\n",
    "data_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://ragudiko%40in.ibm.com:****@na.artifactory.swg-devops.com/artifactory/api/pypi/wcp-nlp-pypi-virtual/simple\n",
      "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.9/site-packages (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/site-packages (from pyLDAvis) (1.22.1)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.9/site-packages (from pyLDAvis) (1.4.0)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.9/site-packages (from pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.9/site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/site-packages (from pyLDAvis) (1.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/site-packages (from pyLDAvis) (1.7.3)\n",
      "Requirement already satisfied: funcy in /usr/local/lib/python3.9/site-packages (from pyLDAvis) (1.17)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.9/site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from pyLDAvis) (3.0.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.9/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pyLDAvis) (57.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas>=1.2.0->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from numexpr->pyLDAvis) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging->numexpr->pyLDAvis) (3.0.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://ragudiko%40in.ibm.com:****@na.artifactory.swg-devops.com/artifactory/api/pypi/wcp-nlp-pypi-virtual/simple\n",
      "Requirement already satisfied: stop_words in /usr/local/lib/python3.9/site-packages (2018.7.23)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stop_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3b2e1c9bbeee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stop_words'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_verb['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\b[^\\d\\W]+\\b'\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "en_stop = get_stop_words('en')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for verb\n",
    "additional_stop_words = ['just','im','amp','virginamerica','united','aa','aaaand', 'abc','abq','yyj','yyz','yr','yrs'\n",
    "                  'yxe','yyc','yvr','zrh','said','let','got','get','say','bked','want','gave','see','dont','know','fyi','vx','using',\n",
    "                         'dallas','iad','told','bos','lax','nyc','flightunited','seems','guy','told','get','go','letsworktogetherunited'\n",
    "                        ,'indicates','make','need','safariunited','mex','take','guy','put','tell','made']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for adj\n",
    "#additional_stop_words = ['dm','unitedunited','whileunited','ur','le','mobile','okc','ual','cant','first','last','next','middle','btwn','sure',\n",
    "#                         'dont','nyc','third','united','ive','itunited','airlinevirginamerica','acceleratevirginamerica','placeunited',\n",
    "#                        'able','amp','free','extra','much','many','unitedairlinesunited','real','lax','fine','early','sfo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in data_verb['reviews'].iteritems():\n",
    "    # clean and tokenize document string\n",
    "    raw = str(i[1]).lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n",
    "    \n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens_new = [raw for raw in stopped_tokens if not raw in additional_stop_words]\n",
    "    \n",
    "    # lemmatize tokens\n",
    "    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens_new]\n",
    "    \n",
    "    # remove word containing only single char\n",
    "    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) <= 2]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(new_lemma_tokens)\n",
    "\n",
    "# sample data\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=50, id2word = dictionary, passes=100,iterations = 500, eval_every=1)\n",
    "#import pprint\n",
    "#pprint.pprint(ldamodel.top_topics(corpus,topn=2))\n",
    "print(ldamodel.print_topics(num_topics=5, num_words=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_in = open(\"pickle/dict_clean.pickle\",\"rb\")\n",
    "dict_clean = pickle.load(pickle_in)\n",
    "dict_clean.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "#Compute Perplexity\n",
    "\n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "print('\\nPerplexity: ', ldamodel.log_perplexity(corpus))\n",
    "\n",
    "#Compute Coherence Score'''\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda_2 = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda_2 = coherence_model_lda_2.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/rajeshgudikoti/Documents/rajesh/learning/data_sets/airline/twitter-airline-sentiment/Tweets.csv\")\n",
    "docs =array(df['text']) ##### to get original review text for c_v option of cohesion score ---but it is giving nan below ---u_mass works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        #coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        #coherencemodel = CoherenceModel(model=model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "        coherencemodel = CoherenceModel(model=model, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=texts, start=2, limit=20, step=1)\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "limit=20; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closer to 8 topics gets u_mass score nearing to zero. The zero value for u_mass score means perfect topic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
